{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0462yiCtYZU-"
      },
      "source": [
        "# Новый раздел"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yz7nVpPma-Yb",
        "outputId": "ef9ab2d5-ce90-4e6f-ccaf-ca76a5e49c81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Oct  5 20:41:45 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   75C    P0    33W /  70W |   1855MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "True\n",
            "/device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "!nvidia-smi\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "print(tf.test.gpu_device_name())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmpUkngLI65g"
      },
      "outputs": [],
      "source": [
        "# %pip install transformers\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('cointegrated/rubert-tiny')\n",
        "\n",
        "\n",
        "class STSBDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, dataset):\n",
        "        # Normalize the similarity scores in the dataset\n",
        "        self.similarity_scores = [float(i) for i in dataset[\"ИндексПохожести\"]]\n",
        "        self.first_sentences = [i for i in dataset[\"НазваниеОписание\"]]\n",
        "        self.second_sentences = [i for i in dataset[\"Категория\"]]\n",
        "        self.concatenated_sentences = [[str(x), str(y)] for x, y in zip(self.first_sentences, self.second_sentences)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.concatenated_sentences)\n",
        "\n",
        "    def get_batch_labels(self, idx):\n",
        "        return torch.tensor(self.similarity_scores[idx])\n",
        "\n",
        "    def get_batch_texts(self, idx):\n",
        "        return tokenizer(self.concatenated_sentences[idx], padding='max_length', max_length=128, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_texts = self.get_batch_texts(idx)\n",
        "        batch_y = self.get_batch_labels(idx)\n",
        "        return batch_texts, batch_y\n",
        "\n",
        "\n",
        "def collate_fn(texts):\n",
        "    input_ids = texts['input_ids']\n",
        "    attention_masks = texts['attention_mask']\n",
        "    features = [{'input_ids': input_id, 'attention_mask': attention_mask}\n",
        "                for input_id, attention_mask in zip(input_ids, attention_masks)]\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CQodNRHI68T"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "class CosineSimilarityLoss(torch.nn.Module):\n",
        "\n",
        "    def __init__(self,  loss_fn=torch.nn.MSELoss(), transform_fn=torch.nn.Identity()):\n",
        "        super(CosineSimilarityLoss, self).__init__()\n",
        "        self.loss_fn = loss_fn\n",
        "        self.transform_fn = transform_fn\n",
        "        self.cos_similarity = torch.nn.CosineSimilarity(dim=1)\n",
        "\n",
        "    def forward(self, inputs, labels):\n",
        "        emb_1 = torch.stack([inp[0] for inp in inputs]).to(device)\n",
        "        emb_2 = torch.stack([inp[1] for inp in inputs]).to(device)\n",
        "        outputs = self.transform_fn(self.cos_similarity(emb_1, emb_2)).to(device)\n",
        "        return self.loss_fn(outputs, labels.squeeze())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5L5mHzT8JIMx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, models\n",
        "\n",
        "'bert-base-uncased'\n",
        "class BertForSTS(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BertForSTS, self).__init__()\n",
        "        self.bert = models.Transformer('cointegrated/rubert-tiny', max_seq_length=128)\n",
        "        self.pooling_layer = models.Pooling(self.bert.get_word_embedding_dimension())\n",
        "        self.sts_bert = SentenceTransformer(modules=[self.bert, self.pooling_layer])\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        \"\"\"\n",
        "        :param input_data: dict('input_ids': tensor([2, 128]), 'attention_mask': tensor([2, 128]))\n",
        "        :return: tensor ([2, 768])\n",
        "        \"\"\"\n",
        "        output = self.sts_bert(input_data)['sentence_embedding'].to(device)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oskYhDwRJIUq",
        "outputId": "9bca4e11-a140-41cb-f4f8-2aa536cdd55b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46EUQMZ4JWUm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "def train(model, device, optimizer, scheduler, train_dataloader, validation_dataloader, epochs=10):\n",
        "    seed_val = 42\n",
        "\n",
        "    criterion = CosineSimilarityLoss()\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    random.seed(seed_val)\n",
        "    torch.manual_seed(seed_val)\n",
        "\n",
        "    training_stats = []\n",
        "\n",
        "    for epoch in range(0, epochs):\n",
        "\n",
        "        # Training\n",
        "\n",
        "        print(\"\")\n",
        "        print('======== Epoch {:} / {:} ========'.format(epoch + 1, epochs))\n",
        "        print('Training...')\n",
        "\n",
        "        total_train_loss = 0\n",
        "\n",
        "        model.train()\n",
        "        # model.eval()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for train_data, train_label in tqdm(train_dataloader):\n",
        "            train_data['input_ids'] = train_data['input_ids'].to(device)\n",
        "            train_data['attention_mask'] = train_data['attention_mask'].to(device)\n",
        "\n",
        "            train_data = collate_fn(train_data)\n",
        "            model.zero_grad()\n",
        "\n",
        "            output = [model(feature) for feature in train_data]\n",
        "\n",
        "            loss = criterion(output, train_label.to(device))\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "        # Calculate the average loss over all the batches.\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"  Average training loss: {0:.5f}\".format(avg_train_loss))\n",
        "\n",
        "        #  Validation\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Running Validation...\")\n",
        "\n",
        "        total_eval_loss = 0\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        # Evaluate data for one epoch\n",
        "        for val_data, val_label in tqdm(validation_dataloader):\n",
        "            val_data['input_ids'] = val_data['input_ids'].to(device)\n",
        "            val_data['attention_mask'] = val_data['attention_mask'].to(device)\n",
        "\n",
        "            val_data = collate_fn(val_data)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                output = [model(feature) for feature in val_data]\n",
        "\n",
        "            loss = criterion(output, val_label.to(device))\n",
        "            total_eval_loss += loss.item()\n",
        "\n",
        "        # Calculate the average loss over all the batches.\n",
        "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "\n",
        "        print(\"  Validation Loss: {0:.5f}\".format(avg_val_loss))\n",
        "\n",
        "        # Record all statistics from this epoch.\n",
        "        training_stats.append(\n",
        "            {\n",
        "                'epoch': epoch + 1,\n",
        "                'Training Loss': avg_train_loss,\n",
        "                'Valid. Loss': avg_val_loss,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    return model, training_stats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "OSL_f3g4JWXc",
        "outputId": "1a38e78e-0304-4b90-e5ba-9f848d9f35d1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 1 ========\n",
            "Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 19/2276 [00:03<07:30,  5.01it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-3e780e34687f>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     model, training_stats = train(model=model.to(device),\n\u001b[0m\u001b[1;32m     64\u001b[0m                                   \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                                   \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-d75433522295>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, optimizer, scheduler, train_dataloader, validation_dataloader, epochs)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mtotal_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "#from testing import predict_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "path_info_with_sim = \"/bert_ozon/data/csv_with_similarity_score/info_with_sim_neg.csv\"\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # if torch.cuda.is_available():\n",
        "    #     device = torch.device(\"cuda\")\n",
        "    # else:\n",
        "    #     device = torch.device(\"cpu\")\n",
        "    device = 0\n",
        "\n",
        "    df = pd.read_csv(path_info_csv, delimiter=',', index_col=False)\n",
        "    train_ds, test_ds = train_test_split(df, test_size=0.2, random_state=0)\n",
        "    train_ds = STSBDataset(train_ds)\n",
        "    test_ds = STSBDataset(test_ds)\n",
        "\n",
        "    model = BertForSTS()\n",
        "\n",
        "    batch_size = 8\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_ds,\n",
        "        num_workers=4,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    validation_dataloader = DataLoader(\n",
        "        test_ds,\n",
        "        num_workers=4,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    # Chose optimizer\n",
        "    optimizer = AdamW(model.parameters(),\n",
        "                      lr=1e-6)\n",
        "\n",
        "    # Create scheduler\n",
        "    epochs = 1\n",
        "    # Total number of training steps is [number of batches] x [number of epochs].\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0,\n",
        "                                                num_training_steps=total_steps)\n",
        "\n",
        "    # Train model\n",
        "    model, training_stats = train(model=model.to(device),\n",
        "                                  device=device,\n",
        "                                  optimizer=optimizer,\n",
        "                                  scheduler=scheduler,\n",
        "                                  train_dataloader=train_dataloader,\n",
        "                                  validation_dataloader=validation_dataloader,\n",
        "                                  epochs=epochs)\n",
        "\n",
        "    model_path_save = \"model.pth\"\n",
        "    torch.save(model.state_dict(), model_path_save)\n",
        "\n",
        "    training_stats_path_save = \"training_stats.txt\"\n",
        "    with open(training_stats_path_save, 'w') as f:\n",
        "        for item in training_stats:\n",
        "            f.writelines(\"%s\\n\" % item)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
